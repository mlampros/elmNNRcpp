[{"path":"https://mlampros.github.io/elmNNRcpp/articles/extreme_learning_machine.html","id":"differences-between-the-elmnn-r-package-and-the-elmnnrcpp-rcpp-package","dir":"Articles","previous_headings":"","what":"Differences between the elmNN (R package) and the elmNNRcpp (Rcpp Package)","title":"Extreme Learning Machine","text":"reimplementation assumes predictors ( x ) response variable ( y ) form matrix. means character, factor boolean columns transformed (onehot encoded option) using either elm_train elm_predict function. output predictions form matrix. case regression matrix one column whereas case classification number columns equals number unique labels case classification unique labels begin 0 difference unique labels greater 1. instance, unique_labels = c(0, 1, 2, 3) acceptable whereas following case raise error : unique_labels = c(0, 2, 3, 4) renamed poslin activation relu ’s easier remember ( share properties ). Moreover added leaky_relu_alpha parameter value greater 0.0 leaky-relu-activation single-hidden-layer can used. initilization weights elmNN set default uniform range [-1,1] ( ‘uniform_negative’ ) . added two options : ‘normal_gaussian’ ( range [0,1] ) ‘uniform_positive’ ( range [0,1] ) user option include exclude bias one-layer feed-forward neural network","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/articles/extreme_learning_machine.html","id":"the-elmnnrcpp-functions","dir":"Articles","previous_headings":"","what":"The elmNNRcpp functions","title":"Extreme Learning Machine","text":"functions included elmNNRcpp package following details parameter can found package documentation,","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/articles/extreme_learning_machine.html","id":"elmnnrcpp-in-case-of-regression","dir":"Articles","previous_headings":"","what":"elmNNRcpp in case of Regression","title":"Extreme Learning Machine","text":"following code chunk gives details use elm_train case regression compares results lm ( linear model ) base function,","code":"# load the data and split it in two parts #----------------------------------------  data(Boston, package = 'KernelKnn')  library(elmNNRcpp) ## Loading required package: KernelKnn Boston = as.matrix(Boston) dimnames(Boston) = NULL  X = Boston[, -dim(Boston)[2]] xtr = X[1:350, ] xte = X[351:nrow(X), ]   # prepare / convert the train-data-response to a one-column matrix #-----------------------------------------------------------------  ytr = matrix(Boston[1:350, dim(Boston)[2]], nrow = length(Boston[1:350, dim(Boston)[2]]),                            ncol = 1)   # perform a fit and predict [ elmNNRcpp ] #----------------------------------------  fit_elm = elm_train(xtr, ytr, nhid = 1000, actfun = 'purelin',                                          init_weights = \"uniform_negative\", bias = TRUE, verbose = T) ## Input weights will be initialized ... ## Dot product of input weights and data starts ... ## Bias will be added to the dot product ... ## 'purelin' activation function will be utilized ... ## The computation of the Moore-Pseudo-inverse starts ... ## The computation is finished! ##  ## Time to complete : 0.04230571 secs pr_te_elm = elm_predict(fit_elm, xte)    # perform a fit and predict [ lm ] #----------------------------------------  data(Boston, package = 'KernelKnn')  fit_lm = lm(medv~., data = Boston[1:350, ])  pr_te_lm = predict(fit_lm, newdata = Boston[351:nrow(X), ])    # evaluation metric #------------------  rmse = function (y_true, y_pred) {      out = sqrt(mean((y_true - y_pred)^2))      out }   # test data response variable #----------------------------  yte = Boston[351:nrow(X), dim(Boston)[2]]   # mean-squared-error for 'elm' and 'lm' #--------------------------------------  cat('the rmse error for extreme-learning-machine is :', rmse(yte, pr_te_elm[, 1]), '\\n') ## the rmse error for extreme-learning-machine is : 23.36543 cat('the rmse error for liner-model is :', rmse(yte, pr_te_lm), '\\n') ## the rmse error for liner-model is : 23.36543"},{"path":"https://mlampros.github.io/elmNNRcpp/articles/extreme_learning_machine.html","id":"elmnnrcpp-in-case-of-classification","dir":"Articles","previous_headings":"","what":"elmNNRcpp in case of Classification","title":"Extreme Learning Machine","text":"following code script illustrates elm_train can used classification compares results glm ( Generalized Linear Models ) base function,","code":"# load the data #--------------  data(ionosphere, package = 'KernelKnn')  y_class = ionosphere[, ncol(ionosphere)]  x_class = ionosphere[, -c(2, ncol(ionosphere))]     # second column has 1 unique value  x_class = scale(x_class[, -ncol(x_class)])  x_class = as.matrix(x_class)                        # convert to matrix dimnames(x_class) = NULL     # split data in train-test #-------------------------  xtr_class = x_class[1:200, ]                     xte_class = x_class[201:nrow(ionosphere), ]  ytr_class = as.numeric(y_class[1:200]) yte_class = as.numeric(y_class[201:nrow(ionosphere)])  ytr_class = onehot_encode(ytr_class - 1)                                     # class labels should begin from 0 (subtract 1)   # perform a fit and predict [ elmNNRcpp ] #----------------------------------------  fit_elm_class = elm_train(xtr_class, ytr_class, nhid = 1000, actfun = 'relu',                                                      init_weights = \"uniform_negative\", bias = TRUE, verbose = TRUE) ## Input weights will be initialized ... ## Dot product of input weights and data starts ... ## Bias will be added to the dot product ... ## 'relu' activation function will be utilized ... ## The computation of the Moore-Pseudo-inverse starts ... ## The computation is finished! ##  ## Time to complete : 0.02375054 secs pr_elm_class = elm_predict(fit_elm_class, xte_class, normalize = FALSE)  pr_elm_class = max.col(pr_elm_class, ties.method = \"random\")    # perform a fit and predict [ glm ] #----------------------------------------  data(ionosphere, package = 'KernelKnn')  fit_glm = glm(class~., data = ionosphere[1:200, -2], family = binomial(link = 'logit')) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred pr_glm = predict(fit_glm, newdata = ionosphere[201:nrow(ionosphere), -2], type = 'response')  pr_glm = as.vector(ifelse(pr_glm < 0.5, 1, 2))   # accuracy for 'elm' and 'glm' #-----------------------------  cat('the accuracy for extreme-learning-machine is :', mean(yte_class == pr_elm_class), '\\n') ## the accuracy for extreme-learning-machine is : 0.9006623 cat('the accuracy for glm is :', mean(yte_class == pr_glm), '\\n') ## the accuracy for glm is : 0.8940397"},{"path":"https://mlampros.github.io/elmNNRcpp/articles/extreme_learning_machine.html","id":"classify-mnist-digits-using-elmnnrcpp","dir":"Articles","previous_headings":"","what":"Classify MNIST digits using elmNNRcpp","title":"Extreme Learning Machine","text":"found interesting Python implementation / Code web thought give try reproduce results. downloaded MNIST data Github repository used following parameter setting,","code":"# using system('wget..') on a linux OS  #-------------------------------------  system(\"wget https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip\")               mnist <- read.table(unz(\"mnist.zip\", \"mnist.csv\"), nrows = 70000, header = T,                                           quote = \"\\\"\", sep = \",\")  x = mnist[, -ncol(mnist)]  y = mnist[, ncol(mnist)]  # using system('wget..') on a linux OS  #-------------------------------------  system(\"wget https://raw.githubusercontent.com/mlampros/DataSets/master/mnist.zip\")               mnist <- read.table(unz(\"mnist.zip\", \"mnist.csv\"), nrows = 70000, header = T,                                           quote = \"\\\"\", sep = \",\")  x = mnist[, -ncol(mnist)]  y = mnist[, ncol(mnist)] + 1   # use the hog-features as input data #-----------------------------------  hog = OpenImageR::HOG_apply(x, cells = 6, orientations = 9, rows = 28, columns = 28, threads = 6)  y_expand = elmNNRcpp::onehot_encode(y - 1)   # 4-fold cross-validation #------------------------  folds = KernelKnn:::class_folds(folds = 4, as.factor(y)) str(folds)  START = Sys.time()   fit = lapply(1:length(folds), function(x) {      cat('\\n'); cat('fold', x, 'starts ....', '\\n')      tmp_fit = elmNNRcpp::elm_train(as.matrix(hog[unlist(folds[-x]), ]), y_expand[unlist(folds[-x]), ],                                      nhid = 2500, actfun = 'relu', init_weights = 'uniform_negative',                                                                    bias = TRUE, verbose = TRUE)      cat('******************************************', '\\n')      tmp_fit })  END = Sys.time()  END - START  # Time difference of 5.698552 mins   str(fit)   # predictions for 4-fold cross validation #----------------------------------------  test_acc = unlist(lapply(1:length(fit), function(x) {      pr_te = elmNNRcpp::elm_predict(fit[[x]], newdata = as.matrix(hog[folds[[x]], ]))      pr_max_col = max.col(pr_te, ties.method = \"random\")      y_true = max.col(y_expand[folds[[x]], ])      mean(pr_max_col == y_true) }))        test_acc  # [1] 0.9825143 0.9848571 0.9824571 0.9822857   cat('Accuracy ( Mnist data ) :', round(mean(test_acc) * 100, 2), '\\n')  # Accuracy ( Mnist data ) : 98.3"},{"path":"https://mlampros.github.io/elmNNRcpp/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lampros Mouselimis. Author, maintainer. Alberto Gosso. Author. Edwin de Jonge. Contributor.            Github Contributor","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mouselimis L (2025). elmNNRcpp: Extreme Learning Machine Algorithm. R package version 1.0.5, https://CRAN.R-project.org/package=elmNNRcpp. Gosso (2012). elmNN: Implementation ELM (Extreme Learning Machine ) algorithm SLFN ( Single Hidden Layer Feedforward Neural Networks ). Huang G, Zhou H, Ding X, Zhang R (2011). “Extreme Learning Machine Regression Multiclass Classification.” IEEE Transactions Systems, Man, Cybernetics, 42, 513–529. doi:10.1109/TSMCB.2011.2168604. Huang G, Zhu Q, Siew C (2006). “Extreme learning machine: Theory applications.” Neurocomputing, 70, 489–501. doi:10.1016/j.neucom.2005.12.126.","code":"@Manual{,   title = {{elmNNRcpp}: The Extreme Learning Machine Algorithm},   author = {Lampros Mouselimis},   year = {2025},   note = {R package version 1.0.5},   url = {https://CRAN.R-project.org/package=elmNNRcpp}, } @Manual{,   title = {{elmNN}: Implementation of ELM (Extreme Learning Machine ) algorithm for SLFN ( Single Hidden Layer Feedforward Neural Networks )},   author = {Alberto Gosso},   year = {2012}, } @Article{,   title = {Extreme Learning Machine for Regression and Multiclass Classification},   author = {G.-B. Huang and H. Zhou and X. Ding and R. Zhang},   journal = {IEEE Transactions on Systems, Man, and Cybernetics},   year = {2011},   volume = {42},   pages = {513--529},   doi = {10.1109/TSMCB.2011.2168604}, } @Article{,   title = {Extreme learning machine: Theory and applications},   author = {G.-B. Huang and Q.-Y. Zhu and C.-K. Siew},   journal = {Neurocomputing},   year = {2006},   volume = {70},   pages = {489--501},   doi = {10.1016/j.neucom.2005.12.126}, }"},{"path":"https://mlampros.github.io/elmNNRcpp/index.html","id":"elmnnrcpp--extreme-learning-machine-","dir":"","previous_headings":"","what":"elmNNRcpp ( Extreme Learning Machine )","title":"The Extreme Learning Machine Algorithm","text":"elmNNRcpp package reimplementation elmNN using RcppArmadillo elmNN package archived. Based documentation elmNN consists , “Training predict functions SLFN ( Single Hidden-layer Feedforward Neural Networks ) using ELM algorithm. ELM algorithm differs traditional gradient-based algorithms short training times ( doesn’t need iterative tuning, makes learning time fast ) need set parameters like learning rate, momentum, epochs, etc.”. details can found package Documentation, Vignette blog-post. install package CRAN use,  download latest version Github using pak package,   Use following link report bugs/issues, https://github.com/mlampros/elmNNRcpp/issues","code":"install.packages(\"elmNNRcpp\") pak::pak('mlampros/elmNNRcpp')"},{"path":"https://mlampros.github.io/elmNNRcpp/index.html","id":"citation","dir":"","previous_headings":"elmNNRcpp ( Extreme Learning Machine )","what":"Citation:","title":"The Extreme Learning Machine Algorithm","text":"use code repository paper research please cite elmNNRcpp original articles / software https://CRAN.R-project.org/package=elmNNRcpp:","code":"@Manual{,   title = {{elmNNRcpp}: The Extreme Learning Machine Algorithm},   author = {Lampros Mouselimis},   year = {2025},   note = {R package version 1.0.5},   url = {https://CRAN.R-project.org/package=elmNNRcpp}, }"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit an extreme learning model — elm","title":"Fit an extreme learning model — elm","text":"Formula interface elm_train, transforms data frame formula necessary input elm_train, automatically calls onehot_encode classification.","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit an extreme learning model — elm","text":"","code":"elm(   formula,   data,   nhid,   actfun,   init_weights = \"normal_gaussian\",   bias = FALSE,   moorep_pseudoinv_tol = 0.01,   leaky_relu_alpha = 0,   seed = 1,   verbose = FALSE )"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit an extreme learning model — elm","text":"formula formula used specify regression classification. data data.frame data nhid numeric value specifying hidden neurons. Must >= 1 actfun character string specifying type activation function. one following : 'sig' ( sigmoid ), 'sin' ( sine ), 'radbas' ( radial basis ), 'hardlim' ( hard-limit ), 'hardlims' ( symmetric hard-limit ), 'satlins' ( satlins ), 'tansig' ( tan-sigmoid ), 'tribas' ( triangular basis ), 'relu' ( rectifier linear unit ) 'purelin' ( linear ) init_weights character string spcecifying distribution input-weights bias initialized. one following : 'normal_gaussian' (normal / Gaussian distribution zero mean unit variance), 'uniform_positive' ( range [0,1] ) 'uniform_negative' ( range [-1,1] ) bias either TRUE FALSE. TRUE bias weights added hidden layer moorep_pseudoinv_tol numeric value. See references web-link details Moore-Penrose pseudo-inverse specifically pseudo inverse tolerance value leaky_relu_alpha numeric value 0.0 1.0. 0.0 simple relu ( f(x) = 0.0 x < 0, f(x) = x x >= 0 ) activation function used, otherwise leaky-relu ( f(x) = alpha * x x < 0, f(x) = x x >= 0 ). applicable actfun equals 'relu' seed numeric value specifying random seed. Defaults 1 verbose boolean. TRUE information printed console","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit an extreme learning model — elm","text":"elm object can used predict, residuals fitted.","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit an extreme learning model — elm","text":"","code":"elm(Species ~ ., data = iris, nhid = 20, actfun=\"sig\") #> Extreme learning model, elm (classification): #>  #> call:  elm(Species ~ ., data = iris, nhid = 20, actfun = \"sig\")  #> hidden units       : 20  #> activation function: sig  #> accuracy           : 0.9733333  #>  #> confusion matrix : #>             predicted #> observed     setosa versicolor virginica #>   setosa         50          0         0 #>   versicolor      0         48         2 #>   virginica       0          2        48  mod_elm <- elm(Species ~ ., data = iris, nhid = 20, actfun=\"sig\")  # predict classes predict(mod_elm, newdata = iris[1:3,-5]) #> [1] setosa setosa setosa #> Levels: setosa versicolor virginica  # predict probabilities predict(mod_elm, newdata = iris[1:3,-5], type=\"prob\") #>         setosa versicolor   virginica #> [1,] 0.9775201 0.02021516 0.002264738 #> [2,] 0.9519374 0.02871766 0.019344988 #> [3,] 0.9620877 0.02854287 0.009369432  # predict elm output predict(mod_elm, newdata = iris[1:3,-5], type=\"raw\") #>        setosa  versicolor   virginica #> [1,] 1.018682 -0.02106638 0.002360102 #> [2,] 1.009991 -0.03046901 0.020524740 #> [3,] 1.020310 -0.03027020 0.009936443  data(\"Boston\") elm(medv ~ ., data = Boston, nhid = 40, actfun=\"relu\") #> Extreme learning model, elm (regression): #>  #> call:  elm(medv ~ ., data = Boston, nhid = 40, actfun = \"relu\")  #> hidden units       : 40  #> activation function: relu  #> mse                : 20.20491   data(\"ionosphere\") elm(class ~ ., data = ionosphere, nhid=20, actfun=\"relu\") #> Extreme learning model, elm (classification): #>  #> call:  elm(class ~ ., data = ionosphere, nhid = 20, actfun = \"relu\")  #> hidden units       : 20  #> activation function: relu  #> accuracy           : 0.8603989  #>  #> confusion matrix : #>         predicted #> observed   b   g #>        b  89  37 #>        g  12 213"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Extreme Learning Machine predict function — elm_predict","title":"Extreme Learning Machine predict function — elm_predict","text":"Extreme Learning Machine predict function","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extreme Learning Machine predict function — elm_predict","text":"","code":"elm_predict(elm_train_object, newdata, normalize = FALSE)"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extreme Learning Machine predict function — elm_predict","text":"elm_train_object output elm_train function newdata input matrix number columns equal x parameter elm_train function normalize boolean specifying output predictions case classification normalized. TRUE values row output-probability-matrix less 0 greater 1 pushed [0,1] range","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extreme Learning Machine predict function — elm_predict","text":"","code":"library(elmNNRcpp)  #----------- # Regression #-----------  data(Boston, package = 'KernelKnn')  Boston = as.matrix(Boston) dimnames(Boston) = NULL  x = Boston[, -ncol(Boston)] y = matrix(Boston[, ncol(Boston)], nrow = length(Boston[, ncol(Boston)]), ncol = 1)  out_regr = elm_train(x, y, nhid = 20, actfun = 'purelin', init_weights = 'uniform_negative')  pr_regr = elm_predict(out_regr, x)   #--------------- # Classification #---------------  data(ionosphere, package = 'KernelKnn')  x_class = ionosphere[, -c(2, ncol(ionosphere))] x_class = as.matrix(x_class) dimnames(x_class) = NULL  y_class = as.numeric(ionosphere[, ncol(ionosphere)])  y_class_onehot = onehot_encode(y_class - 1)     # class labels should begin from 0  out_class = elm_train(x_class, y_class_onehot, nhid = 20, actfun = 'relu')  pr_class = elm_predict(out_class, x_class, normalize = TRUE)"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Extreme Learning Machine training function — elm_train","title":"Extreme Learning Machine training function — elm_train","text":"Extreme Learning Machine training function","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extreme Learning Machine training function — elm_train","text":"","code":"elm_train(   x,   y,   nhid,   actfun,   init_weights = \"normal_gaussian\",   bias = FALSE,   moorep_pseudoinv_tol = 0.01,   leaky_relu_alpha = 0,   seed = 1,   verbose = FALSE )"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extreme Learning Machine training function — elm_train","text":"x matrix. columns input matrix type numeric y matrix. case regression matrix n rows 1 column. case classification consist n rows n columns, n > 1 equals number unique labels. nhid numeric value specifying hidden neurons. Must >= 1 actfun character string specifying type activation function. one following : 'sig' ( sigmoid ), 'sin' ( sine ), 'radbas' ( radial basis ), 'hardlim' ( hard-limit ), 'hardlims' ( symmetric hard-limit ), 'satlins' ( satlins ), 'tansig' ( tan-sigmoid ), 'tribas' ( triangular basis ), 'relu' ( rectifier linear unit ) 'purelin' ( linear ) init_weights character string spcecifying distribution input-weights bias initialized. one following : 'normal_gaussian' (normal / Gaussian distribution zero mean unit variance), 'uniform_positive' ( range [0,1] ) 'uniform_negative' ( range [-1,1] ) bias either TRUE FALSE. TRUE bias weights added hidden layer moorep_pseudoinv_tol numeric value. See references web-link details Moore-Penrose pseudo-inverse specifically pseudo inverse tolerance value leaky_relu_alpha numeric value 0.0 1.0. 0.0 simple relu ( f(x) = 0.0 x < 0, f(x) = x x >= 0 ) activation function used, otherwise leaky-relu ( f(x) = alpha * x x < 0, f(x) = x x >= 0 ). applicable actfun equals 'relu' seed numeric value specifying random seed. Defaults 1 verbose boolean. TRUE information printed console","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extreme Learning Machine training function — elm_train","text":"input matrix type numeric. means user convert character, factor boolean columns numeric values using elm_train function","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Extreme Learning Machine training function — elm_train","text":"http://arma.sourceforge.net/docs.html https://en.wikipedia.org/wiki/Moore https://www.kaggle.com/robertbm/extreme-learning-machine-example http://rt.dgyblog.com/ml/ml-elm.html","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/elm_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extreme Learning Machine training function — elm_train","text":"","code":"library(elmNNRcpp)  #----------- # Regression #-----------  data(Boston, package = 'KernelKnn')  Boston = as.matrix(Boston) dimnames(Boston) = NULL  x = Boston[, -ncol(Boston)] y = matrix(Boston[, ncol(Boston)], nrow = length(Boston[, ncol(Boston)]), ncol = 1)  out_regr = elm_train(x, y, nhid = 20, actfun = 'purelin', init_weights = 'uniform_negative')   #--------------- # Classification #---------------  data(ionosphere, package = 'KernelKnn')  x_class = ionosphere[, -c(2, ncol(ionosphere))] x_class = as.matrix(x_class) dimnames(x_class) = NULL  y_class = as.numeric(ionosphere[, ncol(ionosphere)])  y_class_onehot = onehot_encode(y_class - 1)     # class labels should begin from 0  out_class = elm_train(x_class, y_class_onehot, nhid = 20, actfun = 'relu')"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/onehot_encode.html","id":null,"dir":"Reference","previous_headings":"","what":"One-hot-encoding of the labels in case of classification — onehot_encode","title":"One-hot-encoding of the labels in case of classification — onehot_encode","text":"One-hot-encoding labels case classification","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/onehot_encode.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"One-hot-encoding of the labels in case of classification — onehot_encode","text":"","code":"onehot_encode(y)"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/onehot_encode.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"One-hot-encoding of the labels in case of classification — onehot_encode","text":"y numeric vector consisting response variable labels. minimum value unique labels begin 0","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/onehot_encode.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"One-hot-encoding of the labels in case of classification — onehot_encode","text":"","code":"library(elmNNRcpp)  y = sample(0:3, 100, replace = TRUE)  y_expand = onehot_encode(y)"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/predict.elm.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict with elm — predict.elm","title":"Predict with elm — predict.elm","text":"Wrapper elm_predict.","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/predict.elm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict with elm — predict.elm","text":"","code":"# S3 method for class 'elm' predict(object, newdata, type = c(\"class\", \"prob\", \"raw\"), ...)"},{"path":"https://mlampros.github.io/elmNNRcpp/reference/predict.elm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict with elm — predict.elm","text":"object elm model fitted elm. newdata data.frame new data type used classification, can either \"class\", \"prob\", \"raw\", class (vector), probability (matrix) output elm function (matrix). ... used","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/reference/predict.elm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict with elm — predict.elm","text":"predicted values","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-105","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.5","title":"elmNNRcpp 1.0.5","text":"CRAN release: 2025-09-15 updated Makevars Makevars.win files adding -DARMA_USE_CURRENT (see issue: https://github.com/RcppCore/RcppArmadillo/issues/476) removed -mthreads compilation option “Makevars.win” file removed “CXX_STD = CXX11” “Makevars” files, “[[Rcpp::plugins(cpp11)]]” “utils.cpp” file due following NOTE CRAN, “NOTE Specified C++11: please drop specification unless essential” (see also: https://www.tidyverse.org/blog/2023/03/cran-checks-compiled-code/#note-regarding-systemrequirements-c11)","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-104","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.4","title":"elmNNRcpp 1.0.4","text":"CRAN release: 2022-01-28 formula interface added function elm (see https://github.com/mlampros/elmNNRcpp/pull/4)","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-103","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.3","title":"elmNNRcpp 1.0.3","text":"CRAN release: 2021-05-04 ’ve added CITATION file inst directory listing papers software used elmNNRcpp package","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-102","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.2","title":"elmNNRcpp 1.0.2","text":"CRAN release: 2020-06-13 fixed documentation ‘normal_gaussian’ weights (see issue: https://github.com/mlampros/elmNNRcpp/issues/1) fixed ‘uniform_negative’ distribution (see issue: https://github.com/mlampros/elmNNRcpp/issues/2)","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-101","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.1","title":"elmNNRcpp 1.0.1","text":"CRAN release: 2018-07-21 added Github repository Url BugReports Url DESCRIPTION file.","code":""},{"path":"https://mlampros.github.io/elmNNRcpp/news/index.html","id":"elmnnrcpp-100","dir":"Changelog","previous_headings":"","what":"elmNNRcpp 1.0.0","title":"elmNNRcpp 1.0.0","text":"CRAN release: 2018-07-05","code":""}]
